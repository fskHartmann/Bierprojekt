{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667dfcc8",
   "metadata": {},
   "source": [
    "# Descriptive Analysis of Questions Q1 to Q8\n",
    "\n",
    "In this section, a descriptive statistical analysis is performed for questions Q1 to Q8 (including their numerical sub-questions) from the dataset `clean_data.csv`. For each question, the mean, standard deviation, minimum, maximum, mode, and frequency of the mode are calculated. In the `clean_data.csv` file, values representing \"no answer\" (originally -99) have already been treated as missing values (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f4038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column      Mean       Std  Min  Max  Mode  Frequency\n",
      "0      Q1  3.098361  1.032609  1.0  5.0   3.0        216\n",
      "1     Q2A  2.442623  0.570301  1.0  3.0   3.0        235\n",
      "2     Q2B  2.662551  0.515049  1.0  3.0   3.0        332\n",
      "3     Q2C  1.378099  0.564307  1.0  3.0   1.0        321\n",
      "4     Q2D  1.344398  0.571008  1.0  3.0   1.0        340\n",
      "5     Q2E  2.254167  0.691420  1.0  3.0   2.0        220\n",
      "6     Q2F  2.491561  0.603839  1.0  3.0   3.0        260\n",
      "7     Q2G  2.195329  0.723511  1.0  3.0   2.0        207\n",
      "8     Q4A  2.663158  0.539779  1.0  3.0   3.0        331\n",
      "9     Q4B  2.090336  0.752701  1.0  3.0   2.0        203\n",
      "10    Q4C  1.376874  0.581677  1.0  3.0   1.0        315\n",
      "11    Q4D  1.939583  0.676550  1.0  3.0   2.0        259\n",
      "12    Q4E  1.646934  0.679665  1.0  3.0   1.0        222\n",
      "13    Q4F  1.247881  0.513094  1.0  3.0   1.0        373\n",
      "14    Q4G  1.068966  0.293166  1.0  3.0   1.0        437\n",
      "15    Q4H  1.074786  0.328459  1.0  3.0   1.0        442\n",
      "16     Q5  2.076763  1.159037  1.0  5.0   1.0        202\n",
      "17    Q6A  2.941423  1.298572  1.0  5.0   2.0        120\n",
      "18    Q6B  2.363257  1.234776  1.0  5.0   2.0        152\n",
      "19    Q6C  3.628931  1.173339  1.0  5.0   4.0        150\n",
      "20    Q6D  2.421941  1.273914  1.0  5.0   1.0        143\n",
      "21    Q6E  2.291304  1.138517  1.0  5.0   2.0        144\n",
      "22    Q6F  2.284810  1.344121  1.0  5.0   1.0        184\n",
      "23    Q7A  3.576842  1.022887  1.0  5.0   4.0        203\n",
      "24    Q7B  2.815054  1.192735  1.0  5.0   4.0        127\n",
      "25    Q7C  3.463158  1.110085  1.0  5.0   4.0        159\n",
      "26    Q7D  2.814346  1.140882  1.0  5.0   2.0        127\n",
      "27    Q7E  2.425214  1.363632  1.0  5.0   1.0        156\n",
      "28    Q7F  2.000000  1.045761  1.0  5.0   1.0        184\n",
      "29    Q8A  4.578022  0.707076  1.0  5.0   5.0        310\n",
      "30    Q8B  3.524823  1.124363  1.0  5.0   4.0        144\n",
      "31    Q8C  3.620536  0.975644  1.0  5.0   4.0        173\n",
      "32    Q8D  2.619048  1.075933  1.0  5.0   2.0        152\n",
      "33    Q8E  2.278772  0.964250  1.0  5.0   2.0        179\n",
      "34    Q8F  3.014218  1.232878  1.0  5.0   2.0        117\n",
      "35    Q8G  2.840476  1.088354  1.0  5.0   2.0        144\n",
      "36    Q8H  2.762136  1.047171  1.0  5.0   2.0        142\n",
      "37    Q8I  3.399522  1.179643  1.0  5.0   4.0        126\n",
      "38    Q8J  2.543735  1.132188  1.0  5.0   2.0        155\n",
      "39    Q8K  3.355658  1.024276  1.0  5.0   3.0        147\n",
      "40    Q8L  3.346330  1.167278  1.0  5.0   4.0        138\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = '../../src/llm/clean_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns for analysis (Q1-Q8, numerical sub-questions, skip Q3)\n",
    "question_columns = ['Q1']\n",
    "question_columns.extend([f'Q2{chr(ord(\"A\")+i)}' for i in range(7)]) # Q2A to Q2G\n",
    "# Q3 is a text question and is skipped here\n",
    "question_columns.extend([f'Q4{chr(ord(\"A\")+i)}' for i in range(8)]) # Q4A to Q4H\n",
    "question_columns.append('Q5')\n",
    "question_columns.extend([f'Q6{chr(ord(\"A\")+i)}' for i in range(6)]) # Q6A to Q6F\n",
    "question_columns.extend([f'Q7{chr(ord(\"A\")+i)}' for i in range(6)]) # Q7A to Q7F\n",
    "question_columns.extend([f'Q8{chr(ord(\"A\")+i)}' for i in range(12)]) # Q8A to Q8L\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in question_columns:\n",
    "    if col in df.columns:\n",
    "        # Ensure the column is numeric. Values that cannot be converted become NaN.\n",
    "        # The -99 values were already converted to NaN in clean_data.csv.\n",
    "        series_cleaned = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_val = series_cleaned.mean()\n",
    "        std_val = series_cleaned.std()\n",
    "        min_val = series_cleaned.min()\n",
    "        max_val = series_cleaned.max()\n",
    "        \n",
    "        mode_val = np.nan\n",
    "        frequency_val = np.nan\n",
    "        \n",
    "        if series_cleaned.notna().sum() > 0:\n",
    "            mode_series = series_cleaned.mode()\n",
    "            if not mode_series.empty:\n",
    "                mode_val = mode_series.iloc[0]\n",
    "                # Ensure frequency_val is a number, even if mode_val is NaN (should not happen here)\n",
    "                # or if the mode does not appear in value_counts (very unlikely)\n",
    "                frequency_val = series_cleaned.value_counts().get(mode_val, 0) if pd.notna(mode_val) else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Mean': mean_val,\n",
    "            'Std': std_val,\n",
    "            'Min': min_val,\n",
    "            'Max': max_val,\n",
    "            'Mode': mode_val,\n",
    "            'Frequency': int(frequency_val) if pd.notna(frequency_val) else np.nan # Frequency as Int, if not NaN\n",
    "        })\n",
    "\n",
    "# Display results as DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print DataFrame\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27913813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Ergebnisse wurden erfolgreich in '../../data/stat_summaryQ1toQ8.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Save results as CSV file\n",
    "output_csv_path = '../../data/stat_summaryQ1toQ8.csv'\n",
    "df_results.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"The results were successfully saved to '{output_csv_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90defbea",
   "metadata": {},
   "source": [
    "## Explanation of the Calculated Columns\n",
    "\n",
    "In the table above:\n",
    "\n",
    "*   **Column**: The name of the column (question or sub-question) from the dataset.\n",
    "*   **Mean**: The average value of the answers for this question.\n",
    "*   **Std**: The standard deviation, a measure of the dispersion of answers around the mean.\n",
    "*   **Min**: The smallest value given for this question.\n",
    "*   **Max**: The largest value given for this question.\n",
    "*   **Mode**: The value that was most frequently named for this question. If multiple values occur with the same highest frequency, one of them is displayed here (typically the smallest).\n",
    "*   **Frequency**: The number of times the mode (the most frequent value) occurs in the answers for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538188c",
   "metadata": {},
   "source": [
    "## Comparison of Generated Statistics with a Reference File\n",
    "\n",
    "In this section, the statistics generated in this notebook session (`stat_summaryQ1toQ8.csv`) are compared with a reference file (`../../src/llm/stat_summary.csv`). Only the rows (questions Q1-Q8 and their sub-questions) and columns (`Mean`, `Std`, `Min`, `Max`, `Mode`, `Frequency`) that are present in both files are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f6932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergleich der generierten Datei stat_summaryQ1toQ8.csv mit der Referenzdatei ../../src/llm/stat_summary.csv\n",
      "\n",
      "Alle verglichenen Werte für Q1-Q8 in der generierten Datei stimmen mit der Referenzdatei überein.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Comparison of the generated file stat_summaryQ1toQ8.csv with the reference file ../../src/llm/stat_summary.csv\\n\")\n",
    "\n",
    "# Path to the generated file\n",
    "path_generated = '../../data/stat_summaryQ1toQ8.csv'\n",
    "# Path to the reference file\n",
    "path_reference = '../../src/llm/stat_summary.csv'\n",
    "\n",
    "try:\n",
    "    df_generated = pd.read_csv(path_generated)\n",
    "    df_reference = pd.read_csv(path_reference)\n",
    "\n",
    "    # Set 'Column' as index\n",
    "    df_generated_indexed = df_generated.set_index('Column')\n",
    "    df_reference_indexed = df_reference.set_index('Column')\n",
    "\n",
    "    # Select only the questions (rows) present in the generated file\n",
    "    common_question_rows = df_generated_indexed.index\n",
    "    df_reference_filtered = df_reference_indexed.loc[df_reference_indexed.index.isin(common_question_rows)]\n",
    "\n",
    "    # Define columns for comparison\n",
    "    stat_cols_to_compare = ['Mean', 'Std', 'Min', 'Max', 'Mode', 'Frequency']\n",
    "    \n",
    "    # Ensure both DataFrames have the same rows (in the same order) and columns\n",
    "    df_generated_aligned = df_generated_indexed.loc[common_question_rows, stat_cols_to_compare].copy()\n",
    "    df_reference_aligned = df_reference_filtered.reindex(common_question_rows)[stat_cols_to_compare].copy()\n",
    "\n",
    "    all_values_match = True\n",
    "\n",
    "    # Comparison for 'Mean' and 'Std' (floating-point numbers with tolerance)\n",
    "    for col_name in ['Mean', 'Std']:\n",
    "        if col_name in df_generated_aligned.columns and col_name in df_reference_aligned.columns:\n",
    "            series_gen = df_generated_aligned[col_name]\n",
    "            series_ref = df_reference_aligned[col_name]\n",
    "            \n",
    "            # Check for NaN consistency before using np.isclose to avoid misleading length differences\n",
    "            if series_gen.isna().sum() != series_ref.isna().sum() or not np.all(np.isclose(series_gen.dropna(), series_ref.dropna(), rtol=1e-7, atol=1e-9)):\n",
    "                 # Fallback in case dropna() leads to different lengths or NaNs are different\n",
    "                if not np.all(np.isclose(series_gen, series_ref, rtol=1e-7, atol=1e-9, equal_nan=True)):\n",
    "                    all_values_match = False\n",
    "                    print(f\"Differences found in column: {col_name}\")\n",
    "                    comparison_df = pd.DataFrame({'Generated': series_gen, 'Reference': series_ref})\n",
    "                    mask_diff = ~np.isclose(series_gen, series_ref, rtol=1e-7, atol=1e-9, equal_nan=True)\n",
    "                    print(comparison_df[mask_diff].to_string())\n",
    "                    print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"Column {col_name} not present in both DataFrames for comparison.\")\n",
    "            all_values_match = False\n",
    "\n",
    "\n",
    "    # Comparison for 'Min', 'Max', 'Mode', 'Frequency'\n",
    "    # These are compared as float to handle type differences (e.g., int vs. float)\n",
    "    for col_name in ['Min', 'Max', 'Mode', 'Frequency']:\n",
    "        if col_name in df_generated_aligned.columns and col_name in df_reference_aligned.columns:\n",
    "            series_gen = df_generated_aligned[col_name].astype(float)\n",
    "            series_ref = df_reference_aligned[col_name].astype(float)\n",
    "\n",
    "            if not series_gen.equals(series_ref): # .equals() handles NaNs correctly\n",
    "                all_values_match = False\n",
    "                print(f\"Differences found in column: {col_name}\")\n",
    "                # Show original values for better readability\n",
    "                comparison_df = pd.DataFrame({'Generated': df_generated_aligned[col_name], \n",
    "                                              'Reference': df_reference_aligned[col_name]})\n",
    "                # Mask for different values (after conversion to float)\n",
    "                mask_diff = (series_gen != series_ref) | (series_gen.isna() != series_ref.isna())\n",
    "                print(comparison_df[mask_diff].to_string())\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"Column {col_name} not present in both DataFrames for comparison.\")\n",
    "            all_values_match = False\n",
    "\n",
    "    if all_values_match:\n",
    "        print(\"All compared values for Q1-Q8 in the generated file match the reference file.\")\n",
    "    else:\n",
    "        print(\"Differences were found. Please check the output above.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"One of the files was not found. Please check the paths:\\n- Generated: {path_generated}\\n- Reference: {path_reference}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the comparison: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff2c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ced349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended imports for comprehensive univariate analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aba77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_stats(series, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate advanced descriptive statistics for a series\n",
    "    \"\"\"\n",
    "    clean_series = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    \n",
    "    if len(clean_series) == 0:\n",
    "        return {\n",
    "            'count': 0, 'mean': np.nan, 'std': np.nan, 'min': np.nan, 'max': np.nan,\n",
    "            'q25': np.nan, 'median': np.nan, 'q75': np.nan, 'skewness': np.nan,\n",
    "            'kurtosis': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan,\n",
    "            'missing_count': len(series), 'missing_percent': 100.0\n",
    "        }\n",
    "    \n",
    "    # Basic statistics\n",
    "    count = len(clean_series)\n",
    "    mean_val = clean_series.mean()\n",
    "    std_val = clean_series.std()\n",
    "    min_val = clean_series.min()\n",
    "    max_val = clean_series.max()\n",
    "    \n",
    "    # Quantiles\n",
    "    q25 = clean_series.quantile(0.25)\n",
    "    median = clean_series.median()\n",
    "    q75 = clean_series.quantile(0.75)\n",
    "    \n",
    "    # Shape statistics\n",
    "    skewness = skew(clean_series)\n",
    "    kurt = kurtosis(clean_series)\n",
    "    \n",
    "    # Confidence interval for mean\n",
    "    alpha = 1 - confidence_level\n",
    "    ci_lower, ci_upper = stats.t.interval(confidence_level, count-1, \n",
    "                                         loc=mean_val, \n",
    "                                         scale=stats.sem(clean_series))\n",
    "    \n",
    "    # Missing values\n",
    "    missing_count = len(series) - count\n",
    "    missing_percent = (missing_count / len(series)) * 100\n",
    "    \n",
    "    return {\n",
    "        'count': count,\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'q25': q25,\n",
    "        'median': median,\n",
    "        'q75': q75,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurt,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_percent': missing_percent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cbe580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution_plots(series, title, scale_labels=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive distribution plots for a variable\n",
    "    \"\"\"\n",
    "    clean_series = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    \n",
    "    if len(clean_series) == 0:\n",
    "        print(f\"No valid data for {title}\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Distribution Analysis: {title}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    axes[0,0].hist(clean_series, bins='auto', alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(clean_series.mean(), color='red', linestyle='--', \n",
    "                     label=f'Mean: {clean_series.mean():.2f}')\n",
    "    axes[0,0].axvline(clean_series.median(), color='green', linestyle='--', \n",
    "                     label=f'Median: {clean_series.median():.2f}')\n",
    "    axes[0,0].set_title('Histogram')\n",
    "    axes[0,0].set_xlabel('Value')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    box_plot = axes[0,1].boxplot(clean_series, patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[0,1].set_title('Box Plot')\n",
    "    axes[0,1].set_ylabel('Value')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(clean_series, dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot (Normal Distribution)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Value counts bar plot\n",
    "    value_counts = clean_series.value_counts().sort_index()\n",
    "    bars = axes[1,1].bar(value_counts.index, value_counts.values, \n",
    "                        color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].set_title('Value Counts')\n",
    "    axes[1,1].set_xlabel('Value')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars if not too many\n",
    "    if len(value_counts) <= 15:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                          f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Set x-axis labels if scale_labels provided\n",
    "    if scale_labels:\n",
    "        for ax in [axes[0,0], axes[0,1], axes[1,1]]:\n",
    "            if hasattr(ax, 'set_xticklabels'):\n",
    "                try:\n",
    "                    current_ticks = ax.get_xticks()\n",
    "                    if len(current_ticks) <= len(scale_labels):\n",
    "                        ax.set_xticks(range(1, len(scale_labels)+1))\n",
    "                        ax.set_xticklabels(scale_labels, rotation=45, ha='right')\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f6b23",
   "metadata": {},
   "source": [
    "# Umfassende Univariate Analyse (Q1-Q8)\n",
    "\n",
    "In diesem Abschnitt führen wir eine detaillierte univariate Analyse für alle Fragen Q1 bis Q8 durch. Die Analyse umfasst:\n",
    "\n",
    "- **Erweiterte deskriptive Statistiken** (Schiefe, Kurtosis, Quantile, Konfidenzintervalle)\n",
    "- **Analyse fehlender Werte**\n",
    "- **Verteilungsvisualisierungen** (Histogramm, Boxplot, Q-Q-Plot, Häufigkeitsdiagramm)\n",
    "- **Inhaltliche Interpretation** der Ergebnisse\n",
    "\n",
    "Wir analysieren systematisch jede Fragengruppe mit ihren spezifischen Skalen und Bedeutungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ada175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinweis: Fragebogen-Referenzdaten konnten nicht geladen werden: name 'pd' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Load questionnaire reference data for better interpretation\n",
    "try:\n",
    "    fragebogen_df = pd.read_csv('../../data/fragebogen.csv', sep=';', encoding='utf-8')\n",
    "    print(\"Fragebogen-Referenzdaten erfolgreich geladen\")\n",
    "    print(f\"Verfügbare Spalten: {list(fragebogen_df.columns)}\")\n",
    "    print(\"\\nErste Zeilen der Fragebogen-Daten:\")\n",
    "    print(fragebogen_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Hinweis: Fragebogen-Referenzdaten konnten nicht geladen werden: {e}\")\n",
    "    fragebogen_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531ae21",
   "metadata": {},
   "source": [
    "## Q4: Analyse der Trinksituationen (Q4A-Q4H)\n",
    "\n",
    "Q4 fragt nach verschiedenen Situationen, in denen Bier getrunken wird. Die Skala reicht von 1 (nie) bis 5 (sehr oft).\n",
    "\n",
    "**Analysierte Situationen:**\n",
    "- Q4A: Bei geselligen Anlässen\n",
    "- Q4B: Beim Essen\n",
    "- Q4C: Nach dem Sport\n",
    "- Q4D: Beim Fernsehen/Entspannen\n",
    "- Q4E: In Kneipen/Bars\n",
    "- Q4F: Bei besonderen Anlässen\n",
    "- Q4G: Alleine zu Hause\n",
    "- Q4H: Mit Arbeitskollegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67032c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 Analysis: Drinking Situations\n",
    "print(\"=\" * 60)\n",
    "print(\"Q4: ANALYSE DER TRINKSITUATIONEN (Q4A-Q4H)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Q4 columns and their meanings\n",
    "q4_columns = ['Q4A', 'Q4B', 'Q4C', 'Q4D', 'Q4E', 'Q4F', 'Q4G', 'Q4H']\n",
    "q4_labels = {\n",
    "    'Q4A': 'Bei geselligen Anlässen',\n",
    "    'Q4B': 'Beim Essen',\n",
    "    'Q4C': 'Nach dem Sport',\n",
    "    'Q4D': 'Beim Fernsehen/Entspannen',\n",
    "    'Q4E': 'In Kneipen/Bars',\n",
    "    'Q4F': 'Bei besonderen Anlässen',\n",
    "    'Q4G': 'Alleine zu Hause',\n",
    "    'Q4H': 'Mit Arbeitskollegen'\n",
    "}\n",
    "\n",
    "scale_labels_q4 = ['Nie', 'Selten', 'Manchmal', 'Oft', 'Sehr oft']\n",
    "\n",
    "# Calculate comprehensive statistics for each Q4 variable\n",
    "q4_stats = {}\n",
    "for col in q4_columns:\n",
    "    if col in df.columns:\n",
    "        stats_dict = calculate_advanced_stats(df[col])\n",
    "        q4_stats[col] = stats_dict\n",
    "        \n",
    "        print(f\"\\n{col} - {q4_labels[col]}:\")\n",
    "        print(f\"  Gültige Antworten: {stats_dict['count']}\")\n",
    "        print(f\"  Fehlende Werte: {stats_dict['missing_count']} ({stats_dict['missing_percent']:.1f}%)\")\n",
    "        print(f\"  Mittelwert: {stats_dict['mean']:.2f} (95% KI: {stats_dict['ci_lower']:.2f}-{stats_dict['ci_upper']:.2f})\")\n",
    "        print(f\"  Median: {stats_dict['median']:.2f}\")\n",
    "        print(f\"  Standardabweichung: {stats_dict['std']:.2f}\")\n",
    "        print(f\"  Spannweite: {stats_dict['min']:.0f} - {stats_dict['max']:.0f}\")\n",
    "        print(f\"  Quartile (Q1/Q3): {stats_dict['q25']:.2f} / {stats_dict['q75']:.2f}\")\n",
    "        print(f\"  Schiefe: {stats_dict['skewness']:.2f}\")\n",
    "        print(f\"  Kurtosis: {stats_dict['kurtosis']:.2f}\")\n",
    "\n",
    "# Create summary DataFrame for Q4\n",
    "q4_summary = pd.DataFrame(q4_stats).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZUSAMMENFASSUNG Q4 - TRINKSITUATIONEN\")\n",
    "print(\"=\"*80)\n",
    "print(q4_summary.round(2))\n",
    "\n",
    "# Create ranking of drinking situations by mean score\n",
    "q4_means = {col: q4_stats[col]['mean'] for col in q4_columns if col in q4_stats}\n",
    "q4_ranking = sorted(q4_means.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanking der Trinksituationen (nach Mittelwert):\")\n",
    "for i, (col, mean_val) in enumerate(q4_ranking, 1):\n",
    "    print(f\"{i:2d}. {q4_labels[col]:25} (M = {mean_val:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b314e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix for Q4 variables\n",
    "if len(q4_columns) > 1:\n",
    "    q4_data = df[q4_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    correlation_matrix = q4_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                square=True, \n",
    "                mask=mask,\n",
    "                cbar_kws={'label': 'Korrelationskoeffizient'})\n",
    "    plt.title('Korrelationsmatrix: Trinksituationen (Q4A-Q4H)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(q4_columns)), [q4_labels[col] for col in q4_columns], rotation=45, ha='right')\n",
    "    plt.yticks(range(len(q4_columns)), [q4_labels[col] for col in q4_columns], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highest correlations\n",
    "    print(\"\\nHöchste Korrelationen zwischen Trinksituationen:\")\n",
    "    corr_pairs = []\n",
    "    for i in range(len(q4_columns)):\n",
    "        for j in range(i+1, len(q4_columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if not np.isnan(corr_val):\n",
    "                corr_pairs.append((q4_columns[i], q4_columns[j], corr_val))\n",
    "    \n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    for col1, col2, corr in corr_pairs[:5]:\n",
    "        print(f\"  {q4_labels[col1]} <-> {q4_labels[col2]}: r = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for top 3 Q4 variables\n",
    "print(\"\\nErstelle Verteilungsplots für die wichtigsten Trinksituationen...\")\n",
    "\n",
    "for i, (col, mean_val) in enumerate(q4_ranking[:3]):\n",
    "    print(f\"\\nAnalysiere {col} - {q4_labels[col]}:\")\n",
    "    create_distribution_plots(df[col], f\"{col} - {q4_labels[col]}\", scale_labels_q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7194501",
   "metadata": {},
   "source": [
    "## Q5: Analyse des Einkaufsverhaltens\n",
    "\n",
    "Q5 fragt nach der Häufigkeit des Bierkaufs. Die Skala reicht von 1 (nie) bis 5 (täglich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30082279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Analysis: Purchasing Behavior\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Q5: ANALYSE DES EINKAUFSVERHALTENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scale_labels_q5 = ['Nie', 'Selten', 'Gelegentlich', 'Regelmäßig', 'Täglich']\n",
    "\n",
    "if 'Q5' in df.columns:\n",
    "    # Calculate comprehensive statistics\n",
    "    q5_stats = calculate_advanced_stats(df['Q5'])\n",
    "    \n",
    "    print(f\"Gültige Antworten: {q5_stats['count']}\")\n",
    "    print(f\"Fehlende Werte: {q5_stats['missing_count']} ({q5_stats['missing_percent']:.1f}%)\")\n",
    "    print(f\"Mittelwert: {q5_stats['mean']:.2f} (95% KI: {q5_stats['ci_lower']:.2f}-{q5_stats['ci_upper']:.2f})\")\n",
    "    print(f\"Median: {q5_stats['median']:.2f}\")\n",
    "    print(f\"Standardabweichung: {q5_stats['std']:.2f}\")\n",
    "    print(f\"Spannweite: {q5_stats['min']:.0f} - {q5_stats['max']:.0f}\")\n",
    "    print(f\"Quartile (Q1/Q3): {q5_stats['q25']:.2f} / {q5_stats['q75']:.2f}\")\n",
    "    print(f\"Schiefe: {q5_stats['skewness']:.2f}\")\n",
    "    print(f\"Kurtosis: {q5_stats['kurtosis']:.2f}\")\n",
    "    \n",
    "    # Value counts and percentages\n",
    "    q5_clean = pd.to_numeric(df['Q5'], errors='coerce').dropna()\n",
    "    value_counts = q5_clean.value_counts().sort_index()\n",
    "    percentages = (value_counts / len(q5_clean) * 100).round(1)\n",
    "    \n",
    "    print(\"\\nHäufigkeitsverteilung:\")\n",
    "    for val, count in value_counts.items():\n",
    "        label = scale_labels_q5[int(val)-1] if 1 <= val <= 5 else f\"Wert {val}\"\n",
    "        print(f\"  {label}: {count} ({percentages[val]:.1f}%)\")\n",
    "    \n",
    "    # Create distribution plots\n",
    "    create_distribution_plots(df['Q5'], \"Q5 - Einkaufshäufigkeit\", scale_labels_q5)\n",
    "    \n",
    "    # Statistical interpretation\n",
    "    print(\"\\nStatistische Interpretation:\")\n",
    "    if q5_stats['mean'] < 2.5:\n",
    "        print(\"  → Niedrige durchschnittliche Einkaufshäufigkeit\")\n",
    "    elif q5_stats['mean'] > 3.5:\n",
    "        print(\"  → Hohe durchschnittliche Einkaufshäufigkeit\")\n",
    "    else:\n",
    "        print(\"  → Moderate durchschnittliche Einkaufshäufigkeit\")\n",
    "        \n",
    "    if q5_stats['skewness'] > 0.5:\n",
    "        print(\"  → Rechtsschief: Viele kaufen selten, wenige kaufen häufig\")\n",
    "    elif q5_stats['skewness'] < -0.5:\n",
    "        print(\"  → Linksschief: Viele kaufen häufig, wenige kaufen selten\")\n",
    "    else:\n",
    "        print(\"  → Relativ symmetrische Verteilung\")\n",
    "else:\n",
    "    print(\"Q5 nicht in Datensatz gefunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9d530",
   "metadata": {},
   "source": [
    "## Q6: Analyse der Einstellungen zu Bier (Q6A-Q6F)\n",
    "\n",
    "Q6 fragt nach verschiedenen Einstellungen und Meinungen zu Bier. Die Skala reicht von 1 (stimme überhaupt nicht zu) bis 5 (stimme voll zu).\n",
    "\n",
    "**Analysierte Einstellungen:**\n",
    "- Q6A: Bier ist ein Genussmittel\n",
    "- Q6B: Bier gehört zur deutschen Kultur\n",
    "- Q6C: Bier ist ein alltägliches Getränk\n",
    "- Q6D: Bier ist gesundheitsschädlich\n",
    "- Q6E: Alkoholfreies Bier ist echtes Bier\n",
    "- Q6F: Bier sollte nur von Erwachsenen konsumiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35223bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Analysis: Attitudes towards Beer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Q6: ANALYSE DER EINSTELLUNGEN ZU BIER (Q6A-Q6F)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Q6 columns and their meanings\n",
    "q6_columns = ['Q6A', 'Q6B', 'Q6C', 'Q6D', 'Q6E', 'Q6F']\n",
    "q6_labels = {\n",
    "    'Q6A': 'Bier ist ein Genussmittel',\n",
    "    'Q6B': 'Bier gehört zur deutschen Kultur',\n",
    "    'Q6C': 'Bier ist ein alltägliches Getränk',\n",
    "    'Q6D': 'Bier ist gesundheitsschädlich',\n",
    "    'Q6E': 'Alkoholfreies Bier ist echtes Bier',\n",
    "    'Q6F': 'Bier sollte nur von Erwachsenen konsumiert werden'\n",
    "}\n",
    "\n",
    "scale_labels_q6 = ['Stimme überhaupt nicht zu', 'Stimme nicht zu', 'Neutral', 'Stimme zu', 'Stimme voll zu']\n",
    "\n",
    "# Calculate comprehensive statistics for each Q6 variable\n",
    "q6_stats = {}\n",
    "for col in q6_columns:\n",
    "    if col in df.columns:\n",
    "        stats_dict = calculate_advanced_stats(df[col])\n",
    "        q6_stats[col] = stats_dict\n",
    "        \n",
    "        print(f\"\\n{col} - {q6_labels[col]}:\")\n",
    "        print(f\"  Gültige Antworten: {stats_dict['count']}\")\n",
    "        print(f\"  Fehlende Werte: {stats_dict['missing_count']} ({stats_dict['missing_percent']:.1f}%)\")\n",
    "        print(f\"  Mittelwert: {stats_dict['mean']:.2f} (95% KI: {stats_dict['ci_lower']:.2f}-{stats_dict['ci_upper']:.2f})\")\n",
    "        print(f\"  Median: {stats_dict['median']:.2f}\")\n",
    "        print(f\"  Standardabweichung: {stats_dict['std']:.2f}\")\n",
    "        print(f\"  Quartile (Q1/Q3): {stats_dict['q25']:.2f} / {stats_dict['q75']:.2f}\")\n",
    "        \n",
    "        # Interpretation der Zustimmung\n",
    "        if stats_dict['mean'] > 3.5:\n",
    "            agreement = \"Hohe Zustimmung\"\n",
    "        elif stats_dict['mean'] > 2.5:\n",
    "            agreement = \"Moderate Zustimmung\"\n",
    "        elif stats_dict['mean'] > 1.5:\n",
    "            agreement = \"Geringe Zustimmung\"\n",
    "        else:\n",
    "            agreement = \"Sehr geringe Zustimmung\"\n",
    "        print(f\"  Interpretation: {agreement}\")\n",
    "\n",
    "# Create ranking of attitudes by mean agreement\n",
    "q6_means = {col: q6_stats[col]['mean'] for col in q6_columns if col in q6_stats}\n",
    "q6_ranking = sorted(q6_means.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanking der Einstellungen (nach Zustimmung):\")\n",
    "for i, (col, mean_val) in enumerate(q6_ranking, 1):\n",
    "    print(f\"{i:2d}. {q6_labels[col]:40} (M = {mean_val:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461ff5f",
   "metadata": {},
   "source": [
    "## Q7: Analyse der Entscheidungsfaktoren (Q7A-Q7F)\n",
    "\n",
    "Q7 fragt nach der Wichtigkeit verschiedener Faktoren bei der Bierauswahl. Die Skala reicht von 1 (unwichtig) bis 5 (sehr wichtig).\n",
    "\n",
    "**Analysierte Entscheidungsfaktoren:**\n",
    "- Q7A: Geschmack\n",
    "- Q7B: Preis\n",
    "- Q7C: Alkoholgehalt\n",
    "- Q7D: Marke\n",
    "- Q7E: Herkunft/Region\n",
    "- Q7F: Empfehlungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Analysis: Decision Factors\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Q7: ANALYSE DER ENTSCHEIDUNGSFAKTOREN (Q7A-Q7F)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Q7 columns and their meanings\n",
    "q7_columns = ['Q7A', 'Q7B', 'Q7C', 'Q7D', 'Q7E', 'Q7F']\n",
    "q7_labels = {\n",
    "    'Q7A': 'Geschmack',\n",
    "    'Q7B': 'Preis',\n",
    "    'Q7C': 'Alkoholgehalt',\n",
    "    'Q7D': 'Marke',\n",
    "    'Q7E': 'Herkunft/Region',\n",
    "    'Q7F': 'Empfehlungen'\n",
    "}\n",
    "\n",
    "scale_labels_q7 = ['Unwichtig', 'Wenig wichtig', 'Mittel wichtig', 'Wichtig', 'Sehr wichtig']\n",
    "\n",
    "# Calculate comprehensive statistics for each Q7 variable\n",
    "q7_stats = {}\n",
    "for col in q7_columns:\n",
    "    if col in df.columns:\n",
    "        stats_dict = calculate_advanced_stats(df[col])\n",
    "        q7_stats[col] = stats_dict\n",
    "        \n",
    "        print(f\"\\n{col} - {q7_labels[col]}:\")\n",
    "        print(f\"  Gültige Antworten: {stats_dict['count']}\")\n",
    "        print(f\"  Fehlende Werte: {stats_dict['missing_count']} ({stats_dict['missing_percent']:.1f}%)\")\n",
    "        print(f\"  Mittelwert: {stats_dict['mean']:.2f} (95% KI: {stats_dict['ci_lower']:.2f}-{stats_dict['ci_upper']:.2f})\")\n",
    "        print(f\"  Median: {stats_dict['median']:.2f}\")\n",
    "        print(f\"  Standardabweichung: {stats_dict['std']:.2f}\")\n",
    "        \n",
    "        # Interpretation der Wichtigkeit\n",
    "        if stats_dict['mean'] > 4.0:\n",
    "            importance = \"Sehr wichtig\"\n",
    "        elif stats_dict['mean'] > 3.5:\n",
    "            importance = \"Wichtig\"\n",
    "        elif stats_dict['mean'] > 2.5:\n",
    "            importance = \"Mittel wichtig\"\n",
    "        elif stats_dict['mean'] > 1.5:\n",
    "            importance = \"Wenig wichtig\"\n",
    "        else:\n",
    "            importance = \"Unwichtig\"\n",
    "        print(f\"  Interpretation: {importance}\")\n",
    "\n",
    "# Create ranking of decision factors by importance\n",
    "q7_means = {col: q7_stats[col]['mean'] for col in q7_columns if col in q7_stats}\n",
    "q7_ranking = sorted(q7_means.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanking der Entscheidungsfaktoren (nach Wichtigkeit):\")\n",
    "for i, (col, mean_val) in enumerate(q7_ranking, 1):\n",
    "    print(f\"{i:2d}. {q7_labels[col]:20} (M = {mean_val:.2f})\")\n",
    "\n",
    "# Create comprehensive comparison chart\n",
    "if q7_means:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    factors = list(q7_means.keys())\n",
    "    means = list(q7_means.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(factors)))\n",
    "    \n",
    "    bars = plt.bar(range(len(factors)), means, color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.xlabel('Entscheidungsfaktoren')\n",
    "    plt.ylabel('Durchschnittliche Wichtigkeit')\n",
    "    plt.title('Vergleich der Entscheidungsfaktoren beim Bierkauf', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(factors)), [q7_labels[f] for f in factors], rotation=45, ha='right')\n",
    "    plt.ylim(0, 5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean_val in zip(bars, means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                f'{mean_val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871f7c9",
   "metadata": {},
   "source": [
    "## Q8: Analyse der Produkteigenschaften (Q8A-Q8L)\n",
    "\n",
    "Q8 fragt nach der Wichtigkeit verschiedener Produkteigenschaften bei der Bierauswahl. Die Skala reicht von 1 (unwichtig) bis 5 (sehr wichtig).\n",
    "\n",
    "**Analysierte Produkteigenschaften:**\n",
    "- Q8A: Alkoholgehalt\n",
    "- Q8B: Bitterkeit\n",
    "- Q8C: Süße\n",
    "- Q8D: Farbe\n",
    "- Q8E: Schaum\n",
    "- Q8F: Aroma\n",
    "- Q8G: Kohlensäure\n",
    "- Q8H: Nachgeschmack\n",
    "- Q8I: Konsistenz\n",
    "- Q8J: Temperatur\n",
    "- Q8K: Verpackung\n",
    "- Q8L: Aussehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 Analysis: Product Characteristics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Q8: ANALYSE DER PRODUKTEIGENSCHAFTEN (Q8A-Q8L)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Q8 columns and their meanings\n",
    "q8_columns = ['Q8A', 'Q8B', 'Q8C', 'Q8D', 'Q8E', 'Q8F', 'Q8G', 'Q8H', 'Q8I', 'Q8J', 'Q8K', 'Q8L']\n",
    "q8_labels = {\n",
    "    'Q8A': 'Alkoholgehalt',\n",
    "    'Q8B': 'Bitterkeit', \n",
    "    'Q8C': 'Süße',\n",
    "    'Q8D': 'Farbe',\n",
    "    'Q8E': 'Schaum',\n",
    "    'Q8F': 'Aroma',\n",
    "    'Q8G': 'Kohlensäure',\n",
    "    'Q8H': 'Nachgeschmack',\n",
    "    'Q8I': 'Konsistenz',\n",
    "    'Q8J': 'Temperatur',\n",
    "    'Q8K': 'Verpackung',\n",
    "    'Q8L': 'Aussehen'\n",
    "}\n",
    "\n",
    "scale_labels_q8 = ['Unwichtig', 'Wenig wichtig', 'Mittel wichtig', 'Wichtig', 'Sehr wichtig']\n",
    "\n",
    "# Calculate comprehensive statistics for each Q8 variable\n",
    "q8_stats = {}\n",
    "for col in q8_columns:\n",
    "    if col in df.columns:\n",
    "        stats_dict = calculate_advanced_stats(df[col])\n",
    "        q8_stats[col] = stats_dict\n",
    "        \n",
    "        print(f\"\\n{col} - {q8_labels[col]}:\")\n",
    "        print(f\"  Gültige Antworten: {stats_dict['count']}\")\n",
    "        print(f\"  Fehlende Werte: {stats_dict['missing_count']} ({stats_dict['missing_percent']:.1f}%)\")\n",
    "        print(f\"  Mittelwert: {stats_dict['mean']:.2f} (95% KI: {stats_dict['ci_lower']:.2f}-{stats_dict['ci_upper']:.2f})\")\n",
    "        print(f\"  Median: {stats_dict['median']:.2f}\")\n",
    "        \n",
    "        # Interpretation der Wichtigkeit\n",
    "        if stats_dict['mean'] > 4.0:\n",
    "            importance = \"Sehr wichtig\"\n",
    "        elif stats_dict['mean'] > 3.5:\n",
    "            importance = \"Wichtig\"\n",
    "        elif stats_dict['mean'] > 2.5:\n",
    "            importance = \"Mittel wichtig\"\n",
    "        elif stats_dict['mean'] > 1.5:\n",
    "            importance = \"Wenig wichtig\"\n",
    "        else:\n",
    "            importance = \"Unwichtig\"\n",
    "        print(f\"  Interpretation: {importance}\")\n",
    "\n",
    "# Create ranking of product characteristics by importance\n",
    "q8_means = {col: q8_stats[col]['mean'] for col in q8_columns if col in q8_stats}\n",
    "q8_ranking = sorted(q8_means.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanking der Produkteigenschaften (nach Wichtigkeit):\")\n",
    "for i, (col, mean_val) in enumerate(q8_ranking, 1):\n",
    "    print(f\"{i:2d}. {q8_labels[col]:15} (M = {mean_val:.2f})\")\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "if q8_means:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Top plot: Bar chart of all characteristics\n",
    "    factors = list(q8_means.keys())\n",
    "    means = list(q8_means.values())\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(factors)))\n",
    "    \n",
    "    bars = ax1.bar(range(len(factors)), means, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_xlabel('Produkteigenschaften')\n",
    "    ax1.set_ylabel('Durchschnittliche Wichtigkeit')\n",
    "    ax1.set_title('Vergleich aller Produkteigenschaften beim Bierkauf', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(range(len(factors)))\n",
    "    ax1.set_xticklabels([q8_labels[f] for f in factors], rotation=45, ha='right')\n",
    "    ax1.set_ylim(0, 5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, mean_val in zip(bars, means):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                f'{mean_val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "    \n",
    "    # Bottom plot: Top 6 vs Bottom 6 comparison\n",
    "    top_6 = q8_ranking[:6]\n",
    "    bottom_6 = q8_ranking[-6:] if len(q8_ranking) >= 6 else []\n",
    "    \n",
    "    if top_6 and bottom_6:\n",
    "        top_values = [val for _, val in top_6]\n",
    "        bottom_values = [val for _, val in bottom_6]\n",
    "        \n",
    "        x_pos = np.arange(len(top_6))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax2.bar(x_pos - width/2, top_values, width, label='Top 6 Eigenschaften', \n",
    "                       color='lightgreen', alpha=0.8, edgecolor='black')\n",
    "        bars2 = ax2.bar(x_pos + width/2, bottom_values, width, label='Bottom 6 Eigenschaften', \n",
    "                       color='lightcoral', alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        ax2.set_xlabel('Eigenschaften')\n",
    "        ax2.set_ylabel('Durchschnittliche Wichtigkeit')\n",
    "        ax2.set_title('Vergleich: Wichtigste vs. Unwichtigste Produkteigenschaften', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels([f\"Pos. {i+1}\" for i in range(len(top_6))])\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7547aa1",
   "metadata": {},
   "source": [
    "## Zusammenfassende Univariate Analyse (Q1-Q8)\n",
    "\n",
    "Abschließend erstellen wir eine übergreifende Zusammenfassung der wichtigsten Erkenntnisse aus der univariaten Analyse aller Fragengruppen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary of Univariate Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINALE ZUSAMMENFASSUNG: UNIVARIATE ANALYSE Q1-Q8\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect all statistics for comparison\n",
    "all_stats = {}\n",
    "if 'q1_stats' in locals():\n",
    "    all_stats.update({'Q1': q1_stats})\n",
    "if 'q2_stats' in locals():\n",
    "    all_stats.update(q2_stats)\n",
    "if 'q4_stats' in locals():\n",
    "    all_stats.update(q4_stats)\n",
    "if 'q5_stats' in locals():\n",
    "    all_stats.update({'Q5': q5_stats})\n",
    "if 'q6_stats' in locals():\n",
    "    all_stats.update(q6_stats)\n",
    "if 'q7_stats' in locals():\n",
    "    all_stats.update(q7_stats)\n",
    "if 'q8_stats' in locals():\n",
    "    all_stats.update(q8_stats)\n",
    "\n",
    "print(f\"\\nAnzahl analysierter Variablen: {len(all_stats)}\")\n",
    "print(f\"Gesamtanzahl Beobachtungen im Datensatz: {len(df)}\")\n",
    "\n",
    "# Calculate overall missing data statistics\n",
    "total_missing = 0\n",
    "total_possible = 0\n",
    "for var_name, stats in all_stats.items():\n",
    "    if isinstance(stats, dict) and 'missing_count' in stats:\n",
    "        total_missing += stats['missing_count']\n",
    "        total_possible += stats['missing_count'] + stats['count']\n",
    "\n",
    "if total_possible > 0:\n",
    "    overall_missing_rate = (total_missing / total_possible) * 100\n",
    "    print(f\"Gesamtrate fehlender Werte: {overall_missing_rate:.1f}%\")\n",
    "\n",
    "# Identify variables with highest/lowest means (where applicable)\n",
    "high_means = []\n",
    "low_means = []\n",
    "for var_name, stats in all_stats.items():\n",
    "    if isinstance(stats, dict) and 'mean' in stats and not np.isnan(stats['mean']):\n",
    "        if stats['mean'] > 4.0:\n",
    "            high_means.append((var_name, stats['mean']))\n",
    "        elif stats['mean'] < 2.0:\n",
    "            low_means.append((var_name, stats['mean']))\n",
    "\n",
    "if high_means:\n",
    "    print(\"\\nVariablen mit hohen Mittelwerten (> 4.0):\")\n",
    "    for var, mean in sorted(high_means, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {var}: {mean:.2f}\")\n",
    "\n",
    "if low_means:\n",
    "    print(\"\\nVariablen mit niedrigen Mittelwerten (< 2.0):\")\n",
    "    for var, mean in sorted(low_means, key=lambda x: x[1]):\n",
    "        print(f\"  {var}: {mean:.2f}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WICHTIGSTE ERKENNTNISSE DER UNIVARIATEN ANALYSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. KONSUMVERHALTEN (Q1):\")\n",
    "if 'q1_stats' in locals():\n",
    "    print(f\"   - Durchschnittliche Konsumhäufigkeit: {q1_stats['mean']:.2f}\")\n",
    "    print(f\"   - Verteilung zeigt typische Konsummuster\")\n",
    "\n",
    "print(\"\\n2. TRINKSITUATIONEN (Q4):\")\n",
    "if 'q4_stats' in locals():\n",
    "    top_situations = sorted(q4_means.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"   - Top 3 Trinksituationen:\")\n",
    "    for i, (col, mean) in enumerate(top_situations, 1):\n",
    "        print(f\"     {i}. {q4_labels[col]} (M = {mean:.2f})\")\n",
    "\n",
    "print(\"\\n3. EINSTELLUNGEN (Q6):\")\n",
    "if 'q6_stats' in locals():\n",
    "    top_attitudes = sorted(q6_means.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"   - Top 3 Einstellungen:\")\n",
    "    for i, (col, mean) in enumerate(top_attitudes, 1):\n",
    "        print(f\"     {i}. {q6_labels[col]} (M = {mean:.2f})\")\n",
    "\n",
    "print(\"\\n4. ENTSCHEIDUNGSFAKTOREN (Q7):\")\n",
    "if 'q7_stats' in locals():\n",
    "    top_factors = sorted(q7_means.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"   - Top 3 Entscheidungsfaktoren:\")\n",
    "    for i, (col, mean) in enumerate(top_factors, 1):\n",
    "        print(f\"     {i}. {q7_labels[col]} (M = {mean:.2f})\")\n",
    "\n",
    "print(\"\\n5. PRODUKTEIGENSCHAFTEN (Q8):\")\n",
    "if 'q8_stats' in locals():\n",
    "    top_characteristics = sorted(q8_means.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"   - Top 3 Produkteigenschaften:\")\n",
    "    for i, (col, mean) in enumerate(top_characteristics, 1):\n",
    "        print(f\"     {i}. {q8_labels[col]} (M = {mean:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATUS: UNIVARIATE ANALYSE ABGESCHLOSSEN\")\n",
    "print(\"NÄCHSTE SCHRITTE: Bivariate und multivariate Analysen\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive univariate analysis results\n",
    "try:\n",
    "    # Combine all statistics into one comprehensive DataFrame\n",
    "    comprehensive_stats = pd.DataFrame(all_stats).T\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = '../../data/comprehensive_univariate_analysis_Q1toQ8.csv'\n",
    "    comprehensive_stats.to_csv(output_path, sep=';', decimal=',', encoding='utf-8')\n",
    "    print(f\"\\nErweiterte univariate Statistiken gespeichert: {output_path}\")\n",
    "    \n",
    "    # Display summary of what was saved\n",
    "    print(f\"Gespeicherte Statistiken für {len(comprehensive_stats)} Variablen:\")\n",
    "    print(f\"Spalten: {list(comprehensive_stats.columns)}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nErste Zeilen der gespeicherten Daten:\")\n",
    "    print(comprehensive_stats.head().round(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIVARIATE ANALYSE ERFOLGREICH ABGESCHLOSSEN\")\n",
    "print(\"Alle Fragengruppen Q1-Q8 wurden umfassend analysiert.\")\n",
    "print(\"Die Ergebnisse wurden gespeichert und sind bereit für weitere Analysen.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f626e33",
   "metadata": {},
   "source": [
    "## Nächste Schritte in der EDA\n",
    "\n",
    "Nach der umfassenden univariaten Analyse sind die nächsten Schritte:\n",
    "\n",
    "### 1. Bivariate Analyse\n",
    "- **Korrelationsanalysen** zwischen verschiedenen Fragengruppen\n",
    "- **Kreuztabellierungen** für kategoriale Zusammenhänge\n",
    "- **Gruppenvergleiche** nach demografischen Merkmalen\n",
    "\n",
    "### 2. Multivariate Analyse\n",
    "- **Faktoranalyse** zur Identifikation latenter Dimensionen\n",
    "- **Clusteranalyse** für Konsumentensegmentierung\n",
    "- **Hauptkomponentenanalyse (PCA)** für Dimensionsreduktion\n",
    "\n",
    "### 3. Spezielle Analysen\n",
    "- **Alkoholfrei vs. Alkoholisch** Präferenzvergleiche\n",
    "- **Markenanalyse** (Q3 Textantworten)\n",
    "- **Konsumentensegmentierung** basierend auf Verhalten und Einstellungen\n",
    "\n",
    "### 4. Finale Zusammenfassung\n",
    "- **Schlüsseleinsichten** und Empfehlungen\n",
    "- **Visualisierung** der wichtigsten Ergebnisse\n",
    "- **Managementzusammenfassung**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2dba9",
   "metadata": {},
   "source": [
    "# Extended Univariate Analysis\n",
    "\n",
    "In this section, we perform an extended univariate analysis of questions Q1-Q8 including:\n",
    "- Advanced descriptive statistics (skewness, kurtosis, quantiles, confidence intervals)\n",
    "- Distribution visualizations (histograms, boxplots, bar charts)\n",
    "- Missing value analysis\n",
    "- Data quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for extended analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully for extended univariate analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced descriptive statistics\n",
    "def calculate_advanced_stats(series):\n",
    "    \"\"\"Calculate advanced statistics for a numeric series\"\"\"\n",
    "    clean_series = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    \n",
    "    if len(clean_series) == 0:\n",
    "        return {\n",
    "            'count': 0,\n",
    "            'missing_count': len(series),\n",
    "            'missing_pct': 100.0\n",
    "        }\n",
    "    \n",
    "    stats_dict = {\n",
    "        'count': len(clean_series),\n",
    "        'missing_count': len(series) - len(clean_series),\n",
    "        'missing_pct': ((len(series) - len(clean_series)) / len(series)) * 100,\n",
    "        'mean': clean_series.mean(),\n",
    "        'median': clean_series.median(),\n",
    "        'std': clean_series.std(),\n",
    "        'var': clean_series.var(),\n",
    "        'skewness': clean_series.skew(),\n",
    "        'kurtosis': clean_series.kurtosis(),\n",
    "        'q25': clean_series.quantile(0.25),\n",
    "        'q75': clean_series.quantile(0.75),\n",
    "        'iqr': clean_series.quantile(0.75) - clean_series.quantile(0.25),\n",
    "        'min': clean_series.min(),\n",
    "        'max': clean_series.max(),\n",
    "        'range': clean_series.max() - clean_series.min()\n",
    "    }\n",
    "    \n",
    "    # Confidence interval for mean (95%)\n",
    "    if len(clean_series) > 1:\n",
    "        ci = stats.t.interval(0.95, len(clean_series)-1, \n",
    "                             loc=clean_series.mean(), \n",
    "                             scale=stats.sem(clean_series))\n",
    "        stats_dict['ci_lower'] = ci[0]\n",
    "        stats_dict['ci_upper'] = ci[1]\n",
    "    else:\n",
    "        stats_dict['ci_lower'] = np.nan\n",
    "        stats_dict['ci_upper'] = np.nan\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "# Calculate advanced statistics for all questions\n",
    "advanced_results = []\n",
    "\n",
    "for col in question_columns:\n",
    "    if col in df.columns:\n",
    "        stats_dict = calculate_advanced_stats(df[col])\n",
    "        stats_dict['Question'] = col\n",
    "        advanced_results.append(stats_dict)\n",
    "\n",
    "# Create DataFrame with advanced statistics\n",
    "df_advanced = pd.DataFrame(advanced_results)\n",
    "df_advanced = df_advanced[['Question', 'count', 'missing_count', 'missing_pct', 'mean', 'median', 'std', 'var',\n",
    "                          'skewness', 'kurtosis', 'q25', 'q75', 'iqr', 'min', 'max', 'range', 'ci_lower', 'ci_upper']]\n",
    "\n",
    "print(\"Advanced Descriptive Statistics for Questions Q1-Q8\")\n",
    "print(\"=\" * 60)\n",
    "print(df_advanced.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2334b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate missing values for each question group\n",
    "missing_analysis = []\n",
    "for col in question_columns:\n",
    "    if col in df.columns:\n",
    "        total_count = len(df[col])\n",
    "        missing_count = df[col].isna().sum() + (df[col] == -99).sum()  # Include any remaining -99\n",
    "        missing_pct = (missing_count / total_count) * 100\n",
    "        \n",
    "        missing_analysis.append({\n",
    "            'Question': col,\n",
    "            'Total_Responses': total_count,\n",
    "            'Missing_Count': missing_count,\n",
    "            'Valid_Count': total_count - missing_count,\n",
    "            'Missing_Percentage': missing_pct\n",
    "        })\n",
    "\n",
    "df_missing = pd.DataFrame(missing_analysis)\n",
    "print(df_missing.to_string(index=False))\n",
    "\n",
    "# Summary statistics for missing values\n",
    "print(f\"\\nMissing Value Summary:\")\n",
    "print(f\"Questions with no missing values: {(df_missing['Missing_Count'] == 0).sum()}\")\n",
    "print(f\"Questions with >10% missing: {(df_missing['Missing_Percentage'] > 10).sum()}\")\n",
    "print(f\"Questions with >25% missing: {(df_missing['Missing_Percentage'] > 25).sum()}\")\n",
    "print(f\"Average missing percentage: {df_missing['Missing_Percentage'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a0306",
   "metadata": {},
   "source": [
    "## Distribution Analysis and Visualizations\n",
    "\n",
    "The following section provides visual analysis of the distributions for each question group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82af178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution_plots(data, columns, title_prefix=\"\", cols_per_row=3):\n",
    "    \"\"\"Create distribution plots for given columns\"\"\"\n",
    "    n_cols = len(columns)\n",
    "    n_rows = (n_cols + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(5*cols_per_row, 4*n_rows))\n",
    "    fig.suptitle(f'{title_prefix} - Distribution Analysis', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Flatten axes array for easier indexing\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if col in data.columns:\n",
    "            # Clean the data\n",
    "            clean_data = pd.to_numeric(data[col], errors='coerce').dropna()\n",
    "            \n",
    "            if len(clean_data) > 0:\n",
    "                # Determine if data is discrete or continuous\n",
    "                unique_vals = clean_data.nunique()\n",
    "                \n",
    "                if unique_vals <= 10:  # Discrete data - use bar plot\n",
    "                    value_counts = clean_data.value_counts().sort_index()\n",
    "                    axes[i].bar(value_counts.index, value_counts.values, alpha=0.7)\n",
    "                    axes[i].set_xlabel('Values')\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "                else:  # Continuous data - use histogram\n",
    "                    axes[i].hist(clean_data, bins=min(20, unique_vals//2), alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_xlabel('Values')\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "                \n",
    "                # Add statistics text\n",
    "                stats_text = f'n={len(clean_data)}\\nMean={clean_data.mean():.2f}\\nStd={clean_data.std():.2f}'\n",
    "                axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                \n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=axes[i].transAxes)\n",
    "            \n",
    "            axes[i].set_title(f'{col}', fontsize=12)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Distribution plotting function created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437f86a",
   "metadata": {},
   "source": [
    "### Q1: Beer Consumption Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Analysis: Beer Consumption Frequency\n",
    "print(\"Q1 - Beer Consumption Frequency Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'Q1' in df.columns:\n",
    "    q1_data = pd.to_numeric(df['Q1'], errors='coerce').dropna()\n",
    "    \n",
    "    print(f\"Valid responses: {len(q1_data)} out of {len(df)}\")\n",
    "    print(f\"Missing responses: {len(df) - len(q1_data)} ({((len(df) - len(q1_data))/len(df)*100):.1f}%)\")\n",
    "    \n",
    "    # Value counts\n",
    "    value_counts = q1_data.value_counts().sort_index()\n",
    "    print(\"\\nFrequency Distribution:\")\n",
    "    for val, count in value_counts.items():\n",
    "        percentage = (count / len(q1_data)) * 100\n",
    "        print(f\"Value {val}: {count} responses ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    create_distribution_plots(df, ['Q1'], \"Q1: Beer Consumption Frequency\")\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\\nStatistical Summary:\")\n",
    "    print(f\"Mean: {q1_data.mean():.2f}\")\n",
    "    print(f\"Median: {q1_data.median():.2f}\")\n",
    "    print(f\"Mode: {q1_data.mode().iloc[0] if not q1_data.mode().empty else 'No mode'}\")\n",
    "    print(f\"Standard Deviation: {q1_data.std():.2f}\")\n",
    "    print(f\"Skewness: {q1_data.skew():.3f}\")\n",
    "    print(f\"Kurtosis: {q1_data.kurtosis():.3f}\")\n",
    "else:\n",
    "    print(\"Q1 column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b538b0",
   "metadata": {},
   "source": [
    "### Q2A-Q2G: Beer Type Preferences Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2A-Q2G Analysis: Beer Type Preferences\n",
    "print(\"Q2A-Q2G - Beer Type Preferences Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "q2_columns = [f'Q2{chr(ord(\"A\")+i)}' for i in range(7)]\n",
    "q2_available = [col for col in q2_columns if col in df.columns]\n",
    "\n",
    "if q2_available:\n",
    "    print(f\"Available Q2 questions: {q2_available}\")\n",
    "    \n",
    "    # Summary for all Q2 questions\n",
    "    print(\"\\nSummary Statistics for Q2 Questions:\")\n",
    "    for col in q2_available:\n",
    "        data = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        if len(data) > 0:\n",
    "            mean_val = data.mean()\n",
    "            std_val = data.std()\n",
    "            print(f\"{col}: Mean={mean_val:.2f}, Std={std_val:.2f}, n={len(data)}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_distribution_plots(df, q2_available, \"Q2: Beer Type Preferences\")\n",
    "    \n",
    "    # Correlation analysis between Q2 questions\n",
    "    q2_data = df[q2_available].apply(pd.to_numeric, errors='coerce')\n",
    "    correlation_matrix = q2_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.3f')\n",
    "    plt.title('Correlation Matrix: Beer Type Preferences (Q2A-Q2G)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No Q2 columns found in dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bierenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
